{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_df = pd.read_csv('../data/sports_10k.csv', converters={'event_list': literal_eval, 'person_list': literal_eval}).assign(y=lambda x: 2)\n",
    "politics_df = pd.read_csv('../data/politics_10k.csv', converters={'event_list': literal_eval, 'person_list': literal_eval}).assign(y=lambda x: 1)\n",
    "other_df = pd.read_csv('../data/other_10k.csv', converters={'event_list': literal_eval, 'person_list': literal_eval}).assign(y=lambda x: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = sports_df.merge(politics_df, on='id')[['id']].assign(intersection=lambda x: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([sports_df.merge(intersection, how='left').query('intersection != intersection').drop('intersection', axis=1),\n",
    "                politics_df.merge(intersection, how='left').query('intersection != intersection').drop('intersection', axis=1),\n",
    "                other_df],\n",
    "               axis=0)[['title', 'body', 'source', 'y']].assign(dummy=lambda x: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.body = df.body.map(lambda x: x[:100])\n",
    "df = df.fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('y', axis=1), \n",
    "                                                    df.y, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=True, stratify=df.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qthurier/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t_body = Tokenizer(num_words=1E4, lower=False, oov_token='OOV')\n",
    "t_title = Tokenizer(num_words=1E4, lower=False, oov_token='OOV')\n",
    "t_source = Tokenizer(num_words=1E4, lower=False, oov_token='OOV')\n",
    "\n",
    "t_body.fit_on_texts(X_train.body)\n",
    "t_title.fit_on_texts(X_train.title.fillna('missing'))\n",
    "t_source.fit_on_texts(X_train.title.fillna('missing'))\n",
    "\n",
    "enc_body = t_body.texts_to_sequences(X_train.body)\n",
    "enc_title = t_title.texts_to_sequences(X_train.title.fillna('missing'))\n",
    "enc_source = t_source.texts_to_sequences(X_train.source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_body = np.max([len(seq) for seq in enc_body])\n",
    "max_len_title = np.max([len(seq) for seq in enc_title])\n",
    "max_len_source = np.max([len(seq) for seq in enc_source])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_body = pad_sequences(enc_body, maxlen=max_len_body)\n",
    "dat_title = pad_sequences(enc_title, maxlen=max_len_title)\n",
    "dat_source = pad_sequences(enc_source, maxlen=max_len_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Input, Dropout, BatchNormalization\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "body = Input(shape=(max_len_body, )) \n",
    "title = Input(shape=(max_len_title, )) \n",
    "source = Input(shape=(max_len_source, ))\n",
    "\n",
    "body_emb = Embedding(int(t_body.num_words), embedding_dim, input_length=max_len_body)(body)\n",
    "title_emb = Embedding(int(t_title.num_words), embedding_dim, input_length=max_len_title)(title)\n",
    "source_emb = Embedding(int(t_source.num_words), embedding_dim, input_length=max_len_source)(source)\n",
    "\n",
    "cnn_body = Conv1D(150, 3, activation='relu')(body_emb)\n",
    "pool_body = GlobalMaxPooling1D()(cnn_body)\n",
    "\n",
    "cnn_title = Conv1D(150, 3, activation='relu')(title_emb)\n",
    "pool_title = GlobalMaxPooling1D()(cnn_title)\n",
    "\n",
    "cnn_source = Conv1D(150, 3, activation='relu')(source_emb)\n",
    "pool_source = GlobalMaxPooling1D()(cnn_source)\n",
    "\n",
    "concat = concatenate([pool_body, pool_title, pool_source])\n",
    "\n",
    "out = Dense(10, activation='relu')(concat)\n",
    "out = Dropout(0.5)(out)\n",
    "out = Dense(3, activation='softmax')(out)\n",
    "\n",
    "model = Model(inputs=[body, title, source], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.0001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint('../models/keras_early_stopping.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49449, saving model to ../models/keras_early_stopping.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.49449\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.49449\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49449\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.49449\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49449\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a55c9c668>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([dat_body, dat_title, dat_source], y_train, \n",
    "          batch_size=10, epochs=50, validation_split=0.1,\n",
    "          verbose=0, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_body_test = t_body.texts_to_sequences(X_test.body)\n",
    "enc_title_test = t_title.texts_to_sequences(X_test.title)\n",
    "enc_source_test = t_source.texts_to_sequences(X_test.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_body_test = pad_sequences(enc_body_test, maxlen=max_len_body)\n",
    "dat_title_test = pad_sequences(enc_title_test, maxlen=max_len_title)\n",
    "dat_source_test = pad_sequences(enc_source_test, maxlen=max_len_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict([dat_body_test, dat_title_test, dat_source_test]).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8889712727673142"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_hat, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
