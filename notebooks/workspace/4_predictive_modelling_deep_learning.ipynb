{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook purpose\n",
    "This notebook aims to do model selection for deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training and test X & y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/'\n",
    "models_path = '../../models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(f'{data_path}X_train.csv', converters={'event_list': literal_eval, 'person_list': literal_eval})\n",
    "X_test = pd.read_csv(f'{data_path}X_test.csv', converters={'event_list': literal_eval, 'person_list': literal_eval})\n",
    "y_train = pd.read_csv(f'{data_path}y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv(f'{data_path}y_test.csv').values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare features\n",
    "Here we consider every information as chunk of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we agregate the values in the person and events lists.\n",
    "X_train.event_list = X_train.event_list.map(lambda x: ' '.join(x))\n",
    "X_train.person_list = X_train.person_list.map(lambda x: ' '.join(x))\n",
    "X_test.event_list = X_test.event_list.map(lambda x: ' '.join(x))\n",
    "X_test.person_list = X_test.person_list.map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up tokenizer before encoding text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we aggregate all chunk of text before sending this to the tokenizer\n",
    "all_text_feat = np.concatenate([X_train.body, X_train.title, X_train.source, X_train.event_list, X_train.person_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quentin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(num_words=1E4, lower=False, oov_token='OOV')\n",
    "t.fit_on_texts(all_text_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode chunk of text to sequence of integers and pad the resulting sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_body = t.texts_to_sequences(X_train.body)\n",
    "enc_title = t.texts_to_sequences(X_train.title)\n",
    "enc_source = t.texts_to_sequences(X_train.source)\n",
    "enc_event = t.texts_to_sequences(X_train.event_list)\n",
    "enc_person = t.texts_to_sequences(X_train.person_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_body = np.max([len(seq) for seq in enc_body])\n",
    "max_len_title = np.max([len(seq) for seq in enc_title])\n",
    "max_len_source = np.max([len(seq) for seq in enc_source])\n",
    "max_len_event = np.max([len(seq) for seq in enc_event])\n",
    "max_len_person = np.max([len(seq) for seq in enc_person])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_body = pad_sequences(enc_body, maxlen=max_len_body)\n",
    "dat_title = pad_sequences(enc_title, maxlen=max_len_title)\n",
    "dat_source = pad_sequences(enc_source, maxlen=max_len_source)\n",
    "dat_event = pad_sequences(enc_event, maxlen=max_len_event)\n",
    "dat_person = pad_sequences(enc_person, maxlen=max_len_person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Input, Dropout\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "body = Input(shape=(max_len_body, )) \n",
    "title = Input(shape=(max_len_title, )) \n",
    "source = Input(shape=(max_len_source, ))\n",
    "person = Input(shape=(max_len_person, ))\n",
    "event = Input(shape=(max_len_event, ))\n",
    "\n",
    "embed = Embedding(int(t.num_words), embedding_dim)\n",
    "\n",
    "cnn_body = Conv1D(150, 3, activation='relu')(embed(body))\n",
    "pool_body = GlobalMaxPooling1D()(cnn_body)\n",
    "\n",
    "cnn_title = Conv1D(150, 3, activation='relu')(embed(title))\n",
    "pool_title = GlobalMaxPooling1D()(cnn_title)\n",
    "\n",
    "cnn_source = Conv1D(150, 3, activation='relu')(embed(source))\n",
    "pool_source = GlobalMaxPooling1D()(cnn_source)\n",
    "\n",
    "cnn_event = Conv1D(150, 3, activation='relu')(embed(event))\n",
    "pool_event = GlobalMaxPooling1D()(cnn_event)\n",
    "\n",
    "cnn_person = Conv1D(150, 3, activation='relu')(embed(person))\n",
    "pool_person = GlobalMaxPooling1D()(cnn_person)\n",
    "\n",
    "concat = concatenate([pool_body, pool_title, pool_source, pool_person, pool_event])\n",
    "\n",
    "out = Dense(10, activation='relu')(concat)\n",
    "out = Dropout(0.5)(out)\n",
    "out = Dense(3, activation='softmax')(out)\n",
    "\n",
    "model = Model(inputs=[body, title, source, person, event], outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.0001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define early stoping and checkpoint strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint(f'{models_path}keras_early_stopping.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit model and serialize best weights via early stopping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54020, saving model to ../../models/keras_early_stopping.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.54020 to 0.29927, saving model to ../../models/keras_early_stopping.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.29927 to 0.22457, saving model to ../../models/keras_early_stopping.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.22457 to 0.18607, saving model to ../../models/keras_early_stopping.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.18607 to 0.18281, saving model to ../../models/keras_early_stopping.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.18281\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.18281\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.18281\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.18281\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.18281\n",
      "Epoch 00010: early stopping\n",
      "CPU times: user 3min 33s, sys: 29.7 s, total: 4min 2s\n",
      "Wall time: 2min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b5a7f5ac8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit([dat_body, dat_title, dat_source, dat_person, dat_event], y_train, \n",
    "          batch_size=10, epochs=50, validation_split=0.1,\n",
    "          verbose=0, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialize architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(f'{models_path}keras_architecture.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_body_test = t.texts_to_sequences(X_test.body)\n",
    "enc_title_test = t.texts_to_sequences(X_test.title)\n",
    "enc_source_test = t.texts_to_sequences(X_test.source)\n",
    "enc_event_test = t.texts_to_sequences(X_test.event_list)\n",
    "enc_person_test = t.texts_to_sequences(X_test.person_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_body_test = pad_sequences(enc_body_test, maxlen=max_len_body)\n",
    "dat_title_test = pad_sequences(enc_title_test, maxlen=max_len_title)\n",
    "dat_source_test = pad_sequences(enc_source_test, maxlen=max_len_source)\n",
    "dat_event_test = pad_sequences(enc_event_test, maxlen=max_len_event)\n",
    "dat_person_test = pad_sequences(enc_person_test, maxlen=max_len_person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialize keras_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_data = {'train': [dat_body, dat_title, dat_source, dat_person, dat_event],\n",
    "              'test': [dat_body_test, dat_title_test, dat_source_test, dat_person_test, dat_event_test]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data_path}keras_data.pkl', 'wb') as f:\n",
    "    pickle.dump(keras_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
